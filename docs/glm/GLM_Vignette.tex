\documentclass[11pt]{article}
\usepackage[english]{babel}
\usepackage{amsmath,amsthm,amsfonts,amssymb,epsfig}
\usepackage[left=1.1in,top=1in,right=1.1in]{geometry}
\usepackage{array}
\usepackage{datetime}
\usepackage{lipsum}
\usepackage{spverbatim}
\usepackage{hyperref}
\hypersetup{colorlinks, urlcolor={blue}}
\usepackage{graphicx}
\graphicspath{ {images/} }

\begin{document}

\thispagestyle{empty} %removes page number

\begin{center}
\textsc{\Large\bf{Generalized Linear Modeling  with H2O's R Package}}
\\
\bigskip
\textsc{November 2014}
\end{center}
\bigskip
\bigskip
\bigskip
\bigskip
\tableofcontents

\newpage


\section{Introduction} \label{1}

H2O's GLM algorithm fits generalized linear model with elastic net penalties. The model fitting computation is distributed and is extremely fast (and scales extremely well) for models with limited number of predictors with non-zero coefficients (~ low thousands). The algorithm can compute model for single value of penlaty argument or full regularization path similar to glmnet. It can compute gaussian (linear), logistic, poisson and gamma regresion models. This document describes usage of H2O's glm in R.

H2O's glm fits the model by solving following problem:

\[ \min\limits_{\beta}\ {{1\over{N}} loglikelihood(family,\beta)  + \lambda (\alpha \| \beta \|_1}  + {1- \alpha \over 2} \| \beta \|_2) \]

The elastic net parameter $\alpha$ controls the penalty distribution between l1 and l2 penalty. It can have any value between 0 and 1. When $\alpha$ = 0, we have no l1 penalty and the problem becomes ridge regression. If $\alpha$ = 1 there is no l2 penalty and we have lasso.

The main advantage of l1 penalty is that it (with sufficiently high $\lambda$) produces sparse solution (l2-only penalty does not reduce coefficients to exactly 0). The two penalties also differ in case of correlated predictors. l2 penalty shrinks ceofficients for correlated columns towards each other, while l1 penalty will pick one and drive the others to zero. Using elastic net argument $\alpha$, you can combine these two behaviors. It is also usefull to always add little l2 penalty to increase numerical stability.

As was mentioned before, model-fitting works and scales well if numcols $<<$ numrows. However, in many cases we want to fit glm model over many predictors (10s of thousands or more) with l1 penalty, selecting only few of them in the end (the rest will have zero coefficient and will not be part of the model). To keep our excellent performance and scaling properties in such cases, we employ strong rules (http://statweb.stanford.edu/~tibs/ftp/strong.pdf) to limit the set of active coefficients. The performance of the algorithm is then limited by the number of coefficients in the model rather than number of coefficients present in the training data set. 

The algorithm can handle categorical variables, which are treated as special kind of sparse vectors - each categorical column is expanded on the fly into multiple binary vectors. Each value in the original vector is mapped into n values, with exactly one being set 1, others are 0. Because the some of the columns catetogiral maps to is always 1, it will be correlated with intercept. To prevent such correlation, we usually skip one of the levels (the first one). This is not necessary with non-zero penalty and h2o has an option to include or factor levels in the model (all levels are always included when computing the whole regularization path). This is usefull e.g. when interpreting the variable importance of the model. 


\subsection{Summary of features} 
In summary, H2O's GLM functionalities include:

\begin{itemize} % @TODO TBD. CURRENTLY SAME AS GBM 
\item fits generalized linear model with elastic net penalty
\item can efficiently compute full regularization path, applies strong rules to limit number of activec predictors and speed up computation
\item efficient handling of categorical variables
\item supports efficient distributed n-fold cross validation
\item supports distributed grid search over elastic-net parameter $\alpha$
\item upper and lower boudns for coefficients
\item proximal operator interface
\end{itemize}



\section{Installation} 

You can load the latest CRAN H2O package by running

\begin{spverbatim}
install.packages("h2o")
\end{spverbatim}
\bigskip
\noindent
Alternatively, you can (and should for this tutorial) download the latest H2O build by following the ``Install in R" instructions in the H2O \href{http://s3.amazonaws.com/h2o-release/h2o/master/latest.html}{download table}. Open your R Console and run the following to install the latest H2O build in R:

\begin{spverbatim}
# The following two commands remove any previously installed H2O packages for R.
if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }

if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }

# Next, we download, install and initialize the H2O package for R (filling in the *'s with the matching digits in the download table)
install.packages("h2o", repos=(c("http://s3.amazonaws.com/h2o-release/h2o/master/
****/R", getOption("repos"))))

library(h2o)

\end{spverbatim}
\noindent
If you are operating on a single node, initialize H2O with

\begin{spverbatim}
h2o_server = h2o.init()

\end{spverbatim}
\noindent
With this command, the H2O R module will start an instance of H2O automatically at localhost:54321. Alternatively,  to specify a connection with an existing H2O cluster node (other than localhost at port 54321) you must explicitly state the IP address and port number in the \texttt{h2o.init()} call. An example is given below, but do not directly paste; you should specify the IP and port number appropriate to your specific environment.

\begin{spverbatim}
h2o_cluster = h2o.init(ip = "192.555.1.123", port = 12345, startH2O = FALSE)

\end{spverbatim}
\noindent
An automatic demo is available to see h2o.gbm at work. Run the following command to observe an example classification model built through H2O's GLM.

\begin{spverbatim}
demo(h2o.glm)
\end{spverbatim}

\subsection{Support} 

Users of the H2O package may submit general enquiries and bug reports to the 0xdata \href{h2ostream@googlegroups.com}{support address}. Alternatively, specific bugs or issues may be filed to the 0xdata \href{https://0xdata.atlassian.net/secure/Dashboard.jspa}{JIRA}.

\section{Generalized Linear Modeling} 
This section contains brief overview of generalized linear models and follows up with few details for each model-family.

Generalized linear model is generalization of linear regression. Linear regression models dependency of response y on vector of predictors x ($y \sim x^T \beta + \beta_0$) with assumption that y has gaussian distribution with mean being linear function of x (+ offset) and deviance $\sigma$ (independent on x), i.e. $ y = \mathcal{N}(x^T \beta + \beta_0 , \sigma^2) $. In many situations, these assumptions are overly restrictive while we would still want to take advantage of linear models. GLM generalizes linear regression in following ways: 
\begin{itemize} 
\item adds non-linear link function, so that $link(y) \sim x^T \beta + \beta_0$.
\item allows variance to depend on predicted value (family argument).
\end{itemize}
This generalization allows us to apply GLM to problems such as binary classificaton (Logistic regression) 
GLM models are fitted by maximizing the likelihood.

\subsection{Model Fitting}
\subsection{Validation}
Evaluating quality of the model is critical part of any data-modelling/prediction task. There are several standard ways how to evalute quality of fitted GLM model. The most common is to judge by deviance statistic. Deviance is based on comparing log likelihood of the fitted model with log likelihood of teh saturated model (theoretical perfect model).

\[ deviance = 2({loglikelihood_{s}} - {loglikelihood_{m}}) \]

H2O's glm computes following validation statistics:
\begin{itemize} 
\item \textbf{null degrees of freedom} number of observations - 1 (the $\beta_0=mean$ is one coefficient)
\item \textbf{degrees of freedom} number of observations - 1 - number of non-zero predictors

\item \textbf{null deviance} residual deviance of null model(predict all to mean). Typically, when evaluating the model, we look at the deviance explained ration, i.e. $ 1 -{ resdev \over nulldev}$.
\item \textbf{residual deviance} deviance of the fitted model
\item \textbf{AIC} The Akaike information criterion (AIC) is a measure of the relative quality of a statistical model for a given set of data. It is usefull for model comparison. Unlike deviance (which would have perfect value for saturated model), it takes into account the complexity of the given model. It is based on meassuring information loss when replacing original data with the model itself. Unlike deviance, it does not meassure absolute quality of the fit (comparison against null-hypothesis).

\item \textbf{AUC} Area under ROC curve
\end{itemize}

\subsubsection{Cross-Validation}
All validation values can be computed either on training data set (the default) or using nfold crossvalidation ($nfolds > 1$). When using nfold cross-validation, we randomly split data into n equally sized parts and train each of the n models on n-1 parts and compute validation on the part which was not used for training.The reported validation parameters are then obtained as follows:
\begin{itemize} 
\item null deviance is sum of null deviances of n-models (each uses null model based on the subset of the data)
\item residual deviance is sum of residual deviances of all n-models.
\item AIC
\item AUC is based on ROC curve build  by summing up confusion matrices built for all n-models.
This means for each threshold, we get confusion matrix which includes all the rows from
the training set, however, each row is classified exclusively by the model
which did not have it in its training set. Computation of AUC itself is then the same as in non-crossvalidatied case. 
\end{itemize}


\subsection{GLM Models} 
Following subsection describe GLM families supported in h2o. 

\subsubsection{Linear Regression (Gaussian family) }
Linear regression refers to gaussian family model. It is the simplest example of GLM, however it has many uses and several advantages over the other families such as faster and more stable computation. 

It models the dependency as purely linear function (with link = identity):
\[ \hat{y} = x^T\beta + \beta_0\]

Model is fitted by solving least squares problem (maximum likelihoood for gaussian family):

\[ \min\limits_{\beta,\beta_0} { {1 \over 2N}\sum\limits_{i=1}\limits^{N}(x_i^{T}\beta  + \beta_0- y_i)^T (x_i^{T}\beta + \beta_0 - y_i))  + \lambda (\alpha \|\beta \|_1 + {1-\alpha \over 2}) \|\beta\|_2} \]


Deviance is simply sum of squared errors:
\[ D = \sum\limits_{i=1}\limits^{N}{(y_i - \hat{y}_i)^2} \]


\textbf{Example}



\subsubsection{Logistic Regression (Binomial Family)}
Logistic regression can be used in case of binary classification problem (response is categorical with two levels). It models dependency as $Pr(y = 1|x)$. Canonical link for binomial family is logit (log of the odds), it's inverse is logistic function which which takes any real number on the input and projects it onto 0,1 range (s-curve). 

\[ \hat{y} = Pr(y = 1|x) = {e^{x^T\beta + \beta_0} \over 1 + e^{x^T \beta + \beta_0} } \]

or alternatively:


\[log {\hat{y} \over 1- \hat{y}} = log{Pr(y=1|x) \over Pr(y=0|x)} = x^T \beta + \beta_0\]

Model is fitted by solving:
\[  \min\limits_{\beta,\beta_0} { {1 \over N}\sum\limits_{i=1}\limits^{N}(y_i (x_i^{T}\beta  + \beta_0) - log (1 + e^{x_i^{T}\beta  + \beta_0} )  + \lambda (\alpha \|\beta \|_1 + {1-\alpha \over 2}) \|\beta\|_2} \]

Deviance is -2 loglikelihood:
\[D = -2\sum\limits_{i=1}\limits^{N}{(y log(\hat{y}) + (1 - y)log(1-\hat{y})  )}\]

\textbf{Decision threshold}

\textbf{Example}

\subsubsection{Poisson Models}
Poisson regression is generally used in cases when response represents counts and we assume errors have Poisson distribution. In general, it can be applied to any data where response is non-negative. 

When building Poisson model, we usually model dependency of the mean on the log scale, i.e. canonical link is log and prediction is:

\[\hat{y} = e^{x^T\beta + \beta_0}\]

Model is fitted by solving:

\[  \min\limits_{\beta,\beta_0} { {1 \over N}\sum\limits_{i=1}\limits^{N}(y_i (x_i^{T}\beta  + \beta_0) - e^{x_i^{T}\beta  + \beta_0})  + \lambda (\alpha \|\beta \|_1 + {1-\alpha \over 2}) \|\beta\|_2} \]

Deviance is 

\[D = -2\sum\limits_{i=1}\limits^{N}{(y log(\hat{y}) - y - \hat{y}}\]


\textbf{Example}

\subsubsection{Gamma Models}
The gamma distribution is useful for modeling a positive continuous response variable, where the conditional variance of the response grows with its mean but where the coefficient of variation of the response $\sigma^2(x)/μ(x)$ is constant for all x - i.e. it has a constant coefficient of variation.

It is usally used with inverse or log link, inverse is the canonical link.


\textbf{Validation}
\textbf{Example}

% Strong Rules paper

\subsubsection{Key parameters}
%@NOTE Cursory definition of features listed in full at end of doc; currently labeled appendix
%Specific parameters to elaborate on?
%—Family
%—Link function
%—Cross Validation
%—Alpha Penalties
%—Strong Rules
%—etc.

\section{Elastic Net Penalty} 
We introduce penalty to model-fitting to avoid over-fitting and to reduce variance of the prediction error. There are two common penalized linear models, Ridge Regression and Lasso. 

Ridge Regression penalizes l2 norm of the vector of coefficients. 

Lasso penalizes l1 norm of the parameter vector.

Elastic net combines the two and adds additional argument $\alpha$ which drives the distribution of the penalty between l1 and l2.

\subsection{Selecting Penalty Parameters}

Since l1 penalty produces sparse solution, penalized glm with $\alpha > 0$ with sufficiently high value of $\lambda$ will produce null model, i.e. model with no non-zero coefficients except the intercept. It can thus be seen that it only makes sense to compute the models for $\lambda < \lambda_{max}$, where $lambda_{max}$ is the smallest $\lambda$ leading to null solution.

\subsubsection{Grid Search}
\subsubsection{Lambda Search}
Lambda search is essentially an automatic efficient grid search over $\lambda$ parameter. It computes glm models for the full regularization path, which is an exponentially dercresing sequence of values, staring at $\lambda_{max}$ and finishing at $\lambda_{min} = \gamma\lambda_{max}, \gamma \in (0,1)$ . $\lambda_{max}$ is defined as the smallest $\lambda$ s.t. the resulting model has no non-zero coefficients other than intercept. Since 



\subsection{Strong Rules}


\section{Use case: Classification with Airline data} \label{3}


\subsection{Airline dataset overview} \label{3.1}

The Airline dataset can be downloaded \href{https://github.com/0xdata/h2o/blob/master/smalldata/airlines/allyears2k_headers.zip}{here}. Remember to save the .csv file to your working directory. Before running the Airline demo we first review how to load data with H2O. 

\subsubsection{Loading data} \label{2.5}

Loading a dataset in R for use with H2O is slightly different from the usual methodology, as we must convert our datasets into \texttt{H2OParsedData} objects. For an example, we use a toy weather dataset which can be downloaded \href{https://raw.githubusercontent.com/0xdata/h2o/master/smalldata/weather.csv}{here}. First load the data to your current working directory in your R Console (do this henceforth for dataset downloads), and then run the following command.
\begin{spverbatim}
weather.hex = h2o.uploadFile(h2o_server, path = "weather.csv", header = TRUE, sep = ",", key = "weather.hex")
\end{spverbatim}
\bigskip
\noindent
To see a quick summary of the data, run the following command.
\begin{spverbatim}
summary(weather.hex)
\end{spverbatim}


\subsection{Performing a trial run} \label{3.2}
Returning to the Airline dataset demo, we first load the dataset with H2O and select which variables we wish to use to predict a chosen response. For example, we may want to model whether flights are delayed based on the departure's scheduled day of the week and day of the month.
\begin{spverbatim}

#Load the data and prepare for modeling
% @TODO h2o_server not found FIXXXXXXX & CHECK VARIABLES
air_train.hex = h2o.uploadFile(h2o_server, path = "AirlinesTrain.csv", header = TRUE, sep = ",", key = "airline_train.hex")

air_test.hex = h2o.uploadFile(h2o_server, path = "AirlinesTest.csv", header = TRUE, sep = ",", key = "airline_test.hex")

x = c("Year", "Month", "DayofMonth", "DayOfWeek", "UniqueCarrier", "FlightNum", "Origin", "Dest", "Distance")
y = "IsArrDelayed" 

\end{spverbatim}

Now we train the GLM model

\begin{spverbatim}
airline.glm <- h2o.glm(x=x, 
                     y=y, 
                     data=airline.train.hex,
                     key = "glm_model",
                     family="binomial",
                     lambda_search = TRUE,
                     return_all_lambda = TRUE,
                     use_all_factor_levels = TRUE,
                     variable_importances = TRUE)
\end{spverbatim}

\subsubsection{Extracting and handling the results} \label{3.2.1}

We can extract the parameters of our model, examine the scoring process, and make predictions on new data.

%@ NOTE typoe in R demo test… i spelled performance wrong :(
\begin{spverbatim}
print("Check performance and AUC")
perf = h2o.performance(air.results$YES,airline.test.hex$IsArrDelayed )
print(perf)
perf@model\$auc
print("Show distribution of predictions with quantile.")
quantile.H2OParsedData(air.results\$YES)  
print("Extract strongest predictions.")
top.air <- h2o.assign(air.results[air.results\$YES > quant[‘75%] ],key="top.air")
top.air
\end{spverbatim}
\noindent
\\
\\
Once we have a satisfactory model, the \texttt{h2o.predict()} command can be used to compute and store predictions on new data, which can then be used for further tasks in the interactive modeling process.
\begin{spverbatim}
#Perform classification on the held out data
prediction = h2o.predict(airline.glm, newdata=air_test.hex)
#Copy predictions from H2O to R
pred = as.data.frame(prediction)
head(pred)
\end{spverbatim}
\subsection{Web interface} \label{3.3}
H2O R users have access to a slick web interface to mirror the model building process in R. After loading data or training a model in R, point your browser to your IP address+port number (e.g., localhost:12345) to launch the web interface. From here you can click on \textsc{Admin} $>$ \textsc{Jobs} to view your specific model details. You can also click on \textsc{Data} $>$ \textsc{View All} to view and keep track of your datasets in current use. 
\subsubsection{Variable importances} \label{3.3.1}
One useful feature is the variable importances option, which can be enabled with the additional argument \texttt{importance=TRUE}. This features allows us to view the absolute and relative predictive strength of each feature in the prediction task. From R, you can access these strengths with the command \texttt{air.model@model\$varimp}. You can also view a visualization of the variable
importances on the web interface.
\subsubsection{Java model} 
Java model is currently not available for GLM.
\\
\\
To download the model open the terminal window, create a directory where the model will be saved, set the new directory as the working directory and follow the curl and java compile commands displayed in the instructions at the top of the java model.

\newpage
\section{Appendix A: Complete parameter list}
\begin{itemize}
\item \texttt{x}: A vector containing the names of the predictors in the model. No default.
\item \texttt{y}: The name of the response variable in the model. No default.
\item \texttt{data}: An \texttt{H2OParsedData} object containing the training data. No default.
\item \texttt{key}: The unique hex key assigned to the resulting model. If none is given, a key will automatically be generated.
\item \texttt{family}: A description of the error distribution and corresponding link function to be used in the model. Currently, Gaussian, binomial, Poisson, gamma, and Tweedie are supported. When a model is specified as Tweedie, users must also specify the appropriate Tweedie power. No default.
\item \texttt{link}: The link function relates the linear predictor to the distribution function. Default is the canonical link for the specified family. The full list of supported links: 
	gaussian: identity, log, inverse 
	binomial: logit, log 
	poisson: log, identity
	gamma: inverse, log, identity
	tweedie: tweedie 
\item \texttt{nfolds}: A logical value indicating whether the algorithm should conduct classification. Otherwise, regression is performed on a numeric response variable.
\item \texttt{nfolds}: Number of folds for cross-validation. Default is 0.
\item \texttt{validation}: An \texttt{H2OParsedData} object indicating the validation dataset used to construct confusion matrix. If left blank, default is the training data.
\item \texttt{alpha}: The elastic-net mixing parameter, which must be in $[0,1]$. The penalty is defined to be $P(\alpha,\beta) = (1-\alpha)/2||\beta||_2^2 + \alpha||\beta||_1 = \sum_j [(1-\alpha)/2 \beta_j^2 + \alpha|\beta_j|] $ so \texttt{alpha=1} is the lasso penalty, while \texttt{alpha=0} is the ridge penalty. Default is 0.5.
\item \texttt{nlambda}: The number of lambda values when performing a search. Default is -1.
\item \texttt{lambda.min.ratio}: Smallest value for lambda as a fraction of lambda.max, the entry value, which is the smallest value for which all coefficients in the model are zero. Default is -1.
\item \texttt{lambda}: The shrinkage parameter, which multiplies $P(\alpha,\beta)$ in the objective. The larger lambda is, the more the coefficients are shrunk toward zero (and each other). Default is 1e-5.
\item \texttt{epsilon}: Number indicating the cutoff for determining if a coefficient is zero. Default is 1e-4.
\item \texttt{standardize}: Logical value indicating whether the data should be standardized (set to mean = 0, variance = 1) before running GLM. Default is true.
\item \texttt{prior}: Prior probability of class 1. Only used if \texttt{family = "binomial"}. Default is the frequency of class 1 in the response column. 
\item \texttt{variable\_importances}: A logical value either TRUE or FALSE to indicate whether the variable importances should be computed.  Compute variable importances for input features. NOTE: If \texttt{use\_all\_factor\_levels} is off the importance of the base level will NOT be shown. Default is false.
\item \texttt{use\_all\_factor\_levels}: A logical value either TRUE or FALSE to indicate whether all factor levels should be used. By default, first factor level is skipped from the possible set of predictors. Set this flag if you want use all of the levels. Note: Needs sufficient regularization to solve! Default is false.
\item \texttt{tweedie.p}: The index of the power variance function for the tweedie distribution. Only used if \texttt{family = “tweedie"}. Default is 1.5.
\item \texttt{iter.max}: Maximum number of iterations allowed. Default is 100.
\item \texttt{higher\_accuracy}: A logical value indicating whether to use line search. This will cause the algorithm to run slower, so generally, it should only be set to TRUE if GLM does not converge otherwise. Default is false.
\item \texttt{lambda\_search}: A logical value indicating whether to conduct a search over the space of lambda values, starting from \texttt{lambda\_max}. When this is set to TRUE, lambda will be interpreted as \texttt{lambda\_min}. Default is false.
\item \texttt{return\_all\_lambda}: A logical value indicating whether to return every model built during the lambda search. Only used if \texttt{lambda\_search = TRUE}. If \texttt{return\_all\_lambda = FALSE}, then only the model corresponding to the optimal lambda will be returned. Default is false.
\item \texttt{max\_predictors}: When \texttt{lambda\_search = TRUE}, the algorithm will stop training if the number of predictors exceeds this value. Ignored when \texttt{ lambda\_search = FALSE} or \texttt{max\_predictors = -1}. Default is -1.
\item \texttt{offset}: Column to be used as an offset, if you have one. No default.
\item \texttt{has\_intercept}: A logical value indicating whether or not to include the intercept term. If there are factor columns in your model, then the intercept must be included. Default is true.

%Additional: 
%Strong Rules Enables: Uses strong rules to filter out inactive columns. Default is true.
%Ignored_cols: No default.
%Non-negative: Restricts coefficients to be non-negative. Default is false.
%parallelism: @Is this just a python thing?

\end{itemize}



\newpage
\section{References}

\end{document}